{"Id":128,"Title":"CURL粗解","Slug":"2014-11-28-1","Text":"# cURL使用\n\n指令：curl\n\n在linux中curl是一个利用URL规则在命令行下工作的文件传输工具，可以说是一款很强大的http命令行工具。它支持文件的上传和下载，是综合传输工具，但按传统，习惯称url为下载工具。\n\n语法:\n\n    $ curl [option] [url]\n\n常见参数：\n\n    -A/--user-agent \u003cstring\u003e              设置用户代理发送给服务器\n    -b/--cookie \u003cname=string/file\u003e        cookie字符串或文件读取位置\n    -c/--cookie-jar \u003cfile\u003e                操作结束后把cookie写入到这个文件中\n    -C/--continue-at \u003coffset\u003e             断点续转\n    -D/--dump-header \u003cfile\u003e               把header信息写入到该文件中\n    -e/--referer                          来源网址\n    -f/--fail                             连接失败时不显示http错误\n    -o/--output                           把输出写到该文件中\n    -O/--remote-name                      把输出写到该文件中，保留远程文件的文件名\n    -r/--range \u003crange\u003e                    检索来自HTTP/1.1或FTP服务器字节范围\n    -s/--silent                           静音模式。不输出任何东西\n    -T/--upload-file \u003cfile\u003e               上传文件\n    -u/--user \u003cuser[:password]\u003e           设置服务器的用户和密码\n    -w/--write-out [format]               什么输出完成后\n    -x/--proxy \u003chost[:port]\u003e              在给定的端口上使用HTTP代理\n    -#/--progress-bar                     进度条显示当前的传送状态\n\n例子：\n##### 0. 服务器端代码（基于python tornado）\n    import tornado.httpserver\n    import tornado.ioloop\n    import tornado.options\n    import tornado.web\n    \n    from tornado.options import define, options\n    \n    import json\n    \n    \n    define(\"port\", default=8888, help=\"run on the given port\", type=int)\n    \n    \n    class MainHandler(tornado.web.RequestHandler):\n        def get(self):\n            arguments = self.request.arguments;\n            print(json.dumps(arguments, indent=2))\n            self.write(\"get hello.\")\n    \n        def post(self):\n            arguments = self.request.arguments;\n            print(json.dumps(arguments, indent=2))\n            self.write(\"post world\")\n    \n        def delete(self):\n            arguments = self.request.arguments;\n            print(json.dumps(arguments, indent=2))\n            self.write(\"delete world\")\n    \n        def patch(self):\n            arguments = self.request.arguments;\n            print(json.dumps(arguments, indent=2))\n            self.write(\"patch world\")\n    \n        def head(self):\n            arguments = self.request.arguments;\n            print(json.dumps(arguments, indent=2))\n            self.write(\"head world\")\n    \n        def put(self):\n            arguments = self.request.arguments;\n            print(json.dumps(arguments, indent=2))\n            self.write(\"put world\")\n    \n        def options(self):\n            arguments = self.request.arguments;\n            print(json.dumps(arguments, indent=2))\n            self.write(\"optons world\")\n    \n    \n    def main():\n        tornado.options.parse_command_line()\n        application = tornado.web.Application([\n            (r\"/\", MainHandler),\n            ])\n        http_server = tornado.httpserver.HTTPServer(application)\n        http_server.listen(options.port)\n        tornado.ioloop.IOLoop.instance().start()\n    \n    \n    if __name__ == \"__main__\":\n        main()\n##### 1、基本用法\n1 基本\n    # curl http://www.example.com\n    get hello.\n2 保存\n    # curl http://www.example.com \u003eout.txt\n    get hello.\n或者\n    # curl -o out.txt http://www.example.com\n    get hello.\n    \n会有如下进度条\n\n    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                     Dload  Upload   Total   Spent    Left  Speed\n    100    10  100    10    0     0    875      0 --:--:-- --:--:-- --:--:--   909\n\n\n保存文件 可以使用curl的内置option:-O(大写)保存网页中的文件\n\n要注意这里后面的url要具体到某个文件，不然抓不下来\n\n    # curl -O http://www.example.com/hello.sh\n\n##### 3. 测试网页返回值\n\n    # curl -o /dev/null -s -w %{http_code} www.example.com\n    200\n\n##### 4、指定proxy服务器以及其端口\n\n很多时候上网需要用到代理服务器(比如是使用代理服务器上网或者因为使用curl别人网站而被别人屏蔽IP地址的时候)，幸运的是curl通过使用内置option：-x来支持设置代理\n\n    # curl -x 192.168.100.100:1080 http://www.example.com\n\n##### 5、cookie\n\n有些网站是使用cookie来记录session信息。对于chrome这样的浏览器，可以轻易处理cookie信息，但在curl中只要增加相关参数也是可以很容易的处理cookie\n\n5.1:保存http的response里面的cookie信息。内置option:-c（小写）\n\n    # curl -c cookiec.txt  http://www.example.com\n\n执行后cookie信息就被存到了cookiec.txt里面了\n\n5.2:保存http的response里面的header信息。内置option: -D\n\n    # curl -D cookied.txt http://www.example.com\n\n执行后cookie信息就被存到了cookied.txt里面了\n\n注意：-c(小写)产生的cookie和-D里面的cookie是不一样的。\n\n5.3:使用cookie\n\n很多网站都是通过监视你的cookie信息来判断你是否按规矩访问他们的网站的，因此我们需要使用保存的cookie信息。内置option: -b\n\n    # curl -b cookiec.txt http://www.example.com\n\n##### 6、模仿浏览器\n\n有些网站需要使用特定的浏览器去访问他们，有些还需要使用某些特定的版本。curl内置option:-A可以让我们指定浏览器去访问网站\n\n    # curl -A \"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.0)\" http://www.example.com\n\n这样服务器端就会认为是使用IE8.0去访问的\n\n##### 7、伪造referer（盗链）\n\n很多服务器会检查http访问的referer从而来控制访问。比如：你是先访问首页，然后再访问首页中的邮箱页面，这里访问邮箱的referer地址就是访问首页成功后的页面地址，如果服务器发现对邮箱页面访问的referer地址不是首页的地址，就断定那是个盗连了\n\ncurl中内置option：-e可以让我们设定referer\n\n    # curl -e \"www.example.com\" http://mail.example.com\n\n这样就会让服务器其以为你是从www.example.com点击某个链接过来的\n\n##### 8、下载文件\n\n8.1：利用curl下载文件。\n\n    #使用内置option：-o(小写)\n    \n    # curl -o dodo1.jpg http:www.example.com/dodo1.JPG\n    \n    #使用内置option：-O（大写)\n    \n    # curl -O http://www.example.com/dodo1.JPG\n\n这样就会以服务器上的名称保存文件到本地\n\n8.2：循环下载\n\n有时候下载图片可以能是前面的部分名称是一样的，就最后的尾椎名不一样\n\n    # curl -O http://www.example.com/dodo[1-5].JPG\n\n这样就会把dodo1，dodo2，dodo3，dodo4，dodo5全部保存下来\n\n8.3：下载重命名\n\n    # curl -O http://www.example.com/{hello,bb}/dodo[1-5].JPG\n\n由于下载的hello与bb中的文件名都是dodo1，dodo2，dodo3，dodo4，dodo5。因此第二次下载的会把第一次下载的覆盖，这样就需要对文件进行重命名。\n\n    # curl -o #1_#2.JPG http://www.example.com/{hello,bb}/dodo[1-5].JPG\n\n这样在hello/dodo1.JPG的文件下载下来就会变成hello_dodo1.JPG,其他文件依此类推，从而有效的避免了文件被覆盖\n\n8.4：分块下载\n\n有时候下载的东西会比较大，这个时候我们可以分段下载。使用内置option：-r\n\n    # curl -r 0-100 -o dodo1_part1.JPG http://www.example.com/dodo1.JPG\n    \n    # curl -r 100-200 -o dodo1_part2.JPG http://www.example.com/dodo1.JPG\n    \n    # curl -r 200- -o dodo1_part3.JPG http://www.example.com/dodo1.JPG\n    \n    # cat dodo1_part* \u003e dodo1.JPG\n\n这样就可以查看dodo1.JPG的内容了\n\n8.5：通过ftp下载文件\n\ncurl可以通过ftp下载文件，curl提供两种从ftp中下载的语法\n\n    # curl -O -u 用户名:密码 ftp://www.example.com/dodo1.JPG\n    \n    # curl -O ftp://用户名:密码@www.example.com/dodo1.JPG\n\n8.6：显示下载进度条\n\n    # curl -# -O http://www.example.com/dodo1.JPG\n\n8.7：不会显示下载进度信息\n\n    # curl -s -O http://www.example.com/dodo1.JPG\n\n##### 9、断点续传\n\n在windows中，我们可以使用迅雷这样的软件进行断点续传。curl可以通过内置option:-C同样可以达到相同的效果\n\n如果在下载dodo1.JPG的过程中突然掉线了，可以使用以下的方式续传\n\n    # curl -C -O http://www.example.com/dodo1.JPG\n\n##### 10、上传文件\n\ncurl不仅仅可以下载文件，还可以上传文件。通过内置option:-T来实现\n\n    # curl -T dodo1.JPG -u 用户名:密码 ftp://www.example.com/img/\n\n这样就向ftp服务器上传了文件dodo1.JPG\n\n##### 11、显示抓取错误\n\n    # curl -f http://www.example.com/error\n\n其他参数(此处翻译为转载)：\n\n    -a/--append                     上传文件时，附加到目标文件\n    --anyauth                       可以使用“任何”身份验证方法\n    --basic                         使用HTTP基本验证\n    -B/--use-ascii                  使用ASCII文本传输\n    -d/--data \u003cdata\u003e                HTTP POST方式传送数据\n    --data-ascii \u003cdata\u003e             以ascii的方式post数据\n    --data-binary \u003cdata\u003e            以二进制的方式post数据\n    --negotiate                     使用HTTP身份验证\n    --digest                        使用数字身份验证\n    --disable-eprt                  禁止使用EPRT或LPRT\n    --disable-epsv                  禁止使用EPSV\n    --egd-file \u003cfile\u003e               为随机数据(SSL)设置EGD socket路径\n    --tcp-nodelay                   使用TCP_NODELAY选项\n    -E/--cert \u003ccert[:passwd]\u003e       客户端证书文件和密码 (SSL)\n    --cert-type \u003ctype\u003e              证书文件类型 (DER/PEM/ENG) (SSL)\n    --key \u003ckey\u003e                     私钥文件名 (SSL)\n    --key-type \u003ctype\u003e               私钥文件类型 (DER/PEM/ENG) (SSL)\n    --pass  \u003cpass\u003e                  私钥密码 (SSL)\n    --engine \u003ceng\u003e                  加密引擎使用 (SSL). \"--engine list\" for list\n    --cacert \u003cfile\u003e                 CA证书 (SSL)\n    --capath \u003cdirectory\u003e            CA目录 (made using c_rehash) to verify peer against (SSL)\n    --ciphers \u003clist\u003e                SSL密码\n    --compressed                    要求返回是压缩的形势 (using deflate or gzip)\n    --connect-timeout \u003cseconds\u003e     设置最大请求时间\n    --create-dirs                   建立本地目录的目录层次结构\n    --crlf                          上传是把LF转变成CRLF\n    --ftp-create-dirs               如果远程目录不存在，创建远程目录\n    --ftp-method [multicwd/nocwd/singlecwd]     控制CWD的使用\n    --ftp-pasv                      使用 PASV/EPSV 代替端口\n    --ftp-skip-pasv-ip              使用PASV的时候,忽略该IP地址\n    --ftp-ssl                       尝试用 SSL/TLS 来进行ftp数据传输\n    --ftp-ssl-reqd                  要求用 SSL/TLS 来进行ftp数据传输\n    -F/--form \u003cname=content\u003e        模拟http表单提交数据\n    -form-string \u003cname=string\u003e      模拟http表单提交数据\n    -g/--globoff                    禁用网址序列和范围使用{}和[]\n    -G/--get                        以get的方式来发送数据\n    -h/--help                       帮助\n    -H/--header \u003cline\u003e              自定义头信息传递给服务器\n    --ignore-content-length         忽略的HTTP头信息的长度 \n    -i/--include                    输出时包括protocol头信息\n    -I/--head                       只显示文档信息\n    -j/--junk-session-cookies       读取文件时忽略session cookie\n    --interface \u003cinterface\u003e         使用指定网络接口/地址\n    --krb4 \u003clevel\u003e                  使用指定安全级别的krb4\n    -k/--insecure                   允许不使用证书到SSL站点\n    -K/--config                     指定的配置文件读取\n    -l/--list-only                  列出ftp目录下的文件名称\n    --limit-rate \u003crate\u003e             设置传输速度\n    --local-port\u003cNUM\u003e               强制使用本地端口号\n    -m/--max-time \u003cseconds\u003e         设置最大传输时间\n    --max-redirs \u003cnum\u003e              设置最大读取的目录数\n    --max-filesize \u003cbytes\u003e          设置最大下载的文件总量\n    -M/--manual                     显示全手动\n    -n/--netrc                      从netrc文件中读取用户名和密码\n    --netrc-optional                使用 .netrc 或者 URL来覆盖-n\n    --ntlm                          使用 HTTP NTLM 身份验证\n    -N/--no-buffer                  禁用缓冲输出\n    -p/--proxytunnel                使用HTTP代理\n    --proxy-anyauth                 选择任一代理身份验证方法\n    --proxy-basic                   在代理上使用基本身份验证\n    --proxy-digest                  在代理上使用数字身份验证\n    --proxy-ntlm                    在代理上使用ntlm身份验证\n    -P/--ftp-port \u003caddress\u003e         使用端口地址，而不是使用PASV\n    -Q/--quote \u003ccmd\u003e                文件传输前，发送命令到服务器\n    --range-file                    读取（SSL）的随机文件\n    -R/--remote-time                在本地生成文件时，保留远程文件时间\n    --retry \u003cnum\u003e                   传输出现问题时，重试的次数\n    --retry-delay \u003cseconds\u003e         传输出现问题时，设置重试间隔时间\n    --retry-max-time \u003cseconds\u003e      传输出现问题时，设置最大重试时间\n    -S/--show-error                 显示错误\n    --socks4 \u003chost[:port]\u003e          用socks4代理给定主机和端口\n    --socks5 \u003chost[:port]\u003e          用socks5代理给定主机和端口\n    -t/--telnet-option \u003cOPT=val\u003e    Telnet选项设置\n    --trace \u003cfile\u003e                  对指定文件进行debug\n    --trace-ascii \u003cfile\u003e            Like --跟踪但没有hex输出\n    --trace-time                    跟踪/详细输出时，添加时间戳\n    --url \u003cURL\u003e                     Spet URL to work with\n    -U/--proxy-user \u003cuser[:password]\u003e  设置代理用户名和密码\n    -V/--version                    显示版本信息\n    -X/--request \u003ccommand\u003e          指定什么命令\n    -y/--speed-time                 放弃限速所要的时间。默认为30\n    -Y/--speed-limit                停止传输速度的限制，速度时间'秒\n    -z/--time-cond                  传送时间设置\n    -0/--http1.0                    使用HTTP 1.0\n    -1/--tlsv1                      使用TLSv1（SSL）\n    -2/--sslv2                      使用SSLv2的（SSL）\n    -3/--sslv3                      使用的SSLv3（SSL）\n    --3p-quote                      like -Q for the source URL for 3rd party transfer\n    --3p-url                        使用url，进行第三方传送\n    --3p-user                       使用用户名和密码，进行第三方传送\n    -4/--ipv4                       使用IP4\n    -6/--ipv6                       使用IP6\n\n### 实战\n使用curl可以下载网络内容，那如何获取curl下载时的下载速度呢，使用下面的命令即可：\n\n    # curl -Lo /dev/null -skw \"%{speed_download}\\n\" http://mirrors.163.com/ubuntu/ls-lR.gz\n    226493.000\n\n当然，还可以获取连接时间、重定向时间等更多的数据：\n\n    # curl -Lo /dev/null -skw \"time_connect: %{time_connect} s\\ntime_namelookup: %{time_namelookup} s\\ntime_pretransfer: %{time_pretransfer} s\\ntime_starttransfer: %{time_starttransfer} s\\ntime_redirect: %{time_redirect} s\\nspeed_download: %{speed_download} B/s\\ntime_total: %{time_total} s\\n\\n\"  http://www.sina.com\n    time_connect: 0.154 s\n    time_namelookup: 0.150 s\n    time_pretransfer: 0.154 s\n    time_starttransfer: 0.163 s\n    time_redirect: 0.157 s\n    speed_download: 324679.000 B/s\n    time_total: 1.692 s\n\n打开百度\n\n    curl http://www.baidu.com\n\n接着你就会看到百度的页面源代码输出。\n\n如果要把这个网页保存下来，可以这样：\n\n    curl http://www.baidu.com \u003e /tmp/baidu.html\n\n你会看到一条进度条，然后源码就被重定向到了/tmp/baidu.html。\n\n或者：\n\n    curl -o /tmp/baidu.html http://www.baidu.com\n\nGET请求\n\n默认直接请求一个url就是发出一个get请求，参数的话直接拼接在url里就好了，如\n\n    curl http://www.baidu.com/s?wd=curl\n\n上述请求会上百度发起一条查询请求，参数是wd=url\n\nPOST请求\n\n    curl -d \"name=test\u0026page=1\" http://www.baidu.com\n\n-d参数指定表单以POST的形式执行。\n\n只展示Header\n\n    curl -I  http://www.baidu.com\n    \n可以看到只返回一些header信息\n\n    HTTP/1.1 200 OK\n    Date: Fri, 07 Nov 2014 09:48:58 GMT\n    Content-Type: text/html; charset=utf-8\n    Connection: Keep-Alive\n    Vary: Accept-Encoding\n    Set-Cookie: BAIDUID=E9DB2F0AC95CB6BFDAD9D5CFDCED0A12:FG=1; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com\n    Set-Cookie: BAIDUPSID=E9DB2F0AC95CB6BFDAD9D5CFDCED0A12; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com\n    Set-Cookie: BDSVRTM=0; path=/\n    Set-Cookie: BD_HOME=0; path=/\n    Set-Cookie: H_PS_PSSID=9725_9165_1465_7800_9452_9498_6504_9509_6018_9700_9757_9531_9478_7798_9453_9793_9024; path=/; domain=.baidu.com\n    P3P: CP=\" OTI DSP COR IVA OUR IND COM \"\n    Cache-Control: private\n    Cxy_all: baidu+3057b288b211c770a1463cc8519b62a8\n    Expires: Fri, 07 Nov 2014 09:48:17 GMT\n    X-Powered-By: HPHP\n    Server: BWS/1.1\n    BDPAGETYPE: 1\n    BDQID: 0xfa28eff900012706\n    BDUSERID: 0\n\n显示通信过程\n\n-v参数可以显示一次http通信的整个过程，包括端口连接和http request头信息\n\n    curl -v www.baidu.com\n    \n    * Adding handle: conn: 0x7ffe4b003a00\n    * Adding handle: send: 0\n    * Adding handle: recv: 0\n    * Curl_addHandleToPipeline: length: 1\n    * - Conn 0 (0x7ffe4b003a00) send_pipe: 1, recv_pipe: 0\n    * About to connect() to www.baidu.com port 80 (#0)\n    *   Trying 61.135.169.125...\n    * Connected to www.baidu.com (61.135.169.125) port 80 (#0)\n    \u003e GET / HTTP/1.1\n    \u003e User-Agent: curl/7.30.0\n    \u003e Host: www.baidu.com\n    \u003e Accept: */*\n    \u003e\n    \u003c HTTP/1.1 200 OK\n    \u003c Date: Fri, 07 Nov 2014 09:49:49 GMT\n    \u003c Content-Type: text/html; charset=utf-8\n    \u003c Transfer-Encoding: chunked\n    \u003c Connection: Keep-Alive\n    \u003c Vary: Accept-Encoding\n    \u003c Set-Cookie: BAIDUID=062E02D23FBB651CF8455B699DF02B64:FG=1; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com\n    \u003c Set-Cookie: BAIDUPSID=062E02D23FBB651CF8455B699DF02B64; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com\n    \u003c Set-Cookie: BDSVRTM=0; path=/\n    \u003c Set-Cookie: BD_HOME=0; path=/\n    \u003c Set-Cookie: H_PS_PSSID=7744_1429_7801_9583_9499_6506_6018_9769_9699_9757_9532_9477_7799_9453_9716_9023; path=/; domain=.baidu.com\n    \u003c P3P: CP=\" OTI DSP COR IVA OUR IND COM \"\n    \u003c Cache-Control: private\n    \u003c Cxy_all: baidu+7dcb6b3c03d32c334d42f311919a14d6\n    \u003c Expires: Fri, 07 Nov 2014 09:49:20 GMT\n    \u003c X-Powered-By: HPHP\n    * Server BWS/1.1 is not blacklisted\n    \u003c Server: BWS/1.1\n    \u003c BDPAGETYPE: 1\n    \u003c BDQID: 0xadb706860000088f\n    \u003c BDUSERID: 0\n\n如果你觉得上面的信息还不够，那么下面的命令可以查看更详细的通信过程。\n\n    curl --trace output.txt www.baidu.com\n    \n或者\n\n    curl --trace-ascii output.txt www.baidu.com\n    \n运行后，请打开output.txt文件查看。\n\nHTTP方法\n\ncurl默认的HTTP方法是GET，使用-X参数可以支持其他动词。\n\n    curl -X POST www.example.com\n\n    curl -X DELETE www.example.com\n\nReferer字段\n\n有时你需要在http request头信息中，提供一个referer字段，表示你是从哪里跳转过来的。\n\n    curl --referer http://www.example.com http://www.example.com\n\nUser Agent字段\n\n这个字段是用来表示客户端的设备信息。服务器有时会根据这个字段，针对不同设备，返回不同格式的网页，比如手机版和桌面版。\n\niPhone4的User Agent是\n\n    Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_0 like Mac OS X; en-us) AppleWebKit/532.9 (KHTML, like Gecko) Version/4.0.5 Mobile/8A293 Safari/6531.22.7\ncurl可以这样模拟：\n\n    curl --user-agent \"[User Agent]\" [URL]\n    \n增加头信息\n\n有时需要在http request之中，自行增加一个头信息。--header参数就可以起到这个作用。\n\n    curl --header \"Content-Type:application/json\" http://example.com","Tags":["linux"],"CreateTime":1417143978,"EditTime":1417143978,"UpdateTime":1417143978,"IsComment":true,"IsLinked":false,"AuthorId":10,"Template":"blog.html","Type":"article","Status":"publish","Format":"markdown","Comments":[],"Hits":399}